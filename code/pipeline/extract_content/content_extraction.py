import torch
import math
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig
import logging
from pathlib import Path
from json import load, dump

from pipeline.extract_content.processing_utils import load_image, postprocess_response

class ContentExtraction:
    """
    Class for content extraction using a large language model.
    """

    def __init__(self, max_new_tokens, cache_folder, load_in_8bit, logger):
        """
        Initialise content extraction object.
        If model files are not found in the cache folder, they are downloaded and saved for reuse.
        The large language models needs GPU to run.
        
        - max_new_tokens (int): Max new tokens generated by the llm
        - cache_folder (str): Folder path to store model cache files
        - load_in_8bit (bool): 8-bit quantization. If set to false, 4-bit quantization will be used.
        - logger (Logger): Logger object
        """

        model = 'OpenGVLab/InternVL2-8B'
        self.logger = logger

        model_cache_folder = Path.joinpath(Path(cache_folder), Path(model.replace(r'/', '-')))

        if load_in_8bit: quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        else: quantization_config = BitsAndBytesConfig(load_in_4bit=True)

        try:
            self.llm = AutoModel.from_pretrained(
                model_cache_folder,
                torch_dtype=torch.bfloat16,
                quantization_config=quantization_config,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                # device_map=self._split_model(),    # uncomment when running on multiple GPUs
                attn_implementation="flash_attention_2"
            )
        except EnvironmentError:
            self.logger.Log("Downloading LLM files!", logging.INFO)
            self.llm = AutoModel.from_pretrained(
                model,
                torch_dtype=torch.bfloat16,
                quantization_config=quantization_config,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                # device_map=self._split_model(),    # uncomment when running on multiple GPUs
                attn_implementation="flash_attention_2",
                cache_dir=model_cache_folder
            )
            self.llm.save_pretrained(model_cache_folder)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True, use_fast=False)

        self.generation_config = dict(max_new_tokens=max_new_tokens)

    def extract_fstring(self, table, use_pipeline_a):
        """
        Method to extract content from the image, and generate a formatted string by passing the preprocessed image along with a prompt to the large language model.
        
        - table (PIL): PIL image of the cropped table.

        * returns tables_image (List): List of tables in the image.
        """

        pixel_values = load_image(table).to(torch.bfloat16).cuda()
        
        if use_pipeline_a:
            question_a1 = "<image>\n Structure the text into a table as shown in the image. Don't include a preamble."
            markdown_response = self.llm.chat(self.tokenizer, pixel_values, question_a1, self.generation_config)
        else:
            question_b1 = "<image>\n Structure the text into a table as shown in the image. Return a json. Return multiple json if there are multiple tables. Don't include a preamble."
            json_response = self.llm.chat(self.tokenizer, pixel_values, question_b1, self.generation_config)
            self.logger.Log(json_response, logging.INFO)

            question_b2 = f"<json>{json_response}</json>\n Convert json to table."
            markdown_response = self.llm.chat(self.tokenizer, pixel_values, question_b2, self.generation_config)

        self.logger.Log(markdown_response, logging.INFO)

        tables_image = postprocess_response(markdown_response)

        [self.logger.Log(f'FString:\n{fstring}', logging.INFO) for fstring in tables_image]
        
        return tables_image

    def save_fstrings(self, strings, output_path, img_name):
        """
        Save fstring in a txt file.
        Read the output file if exists and add new key, value and write to file again.
        If output file doesn't exist, create it.
        
        - strings (List): List of strings to be saved
        - output_path (str): File path to store outputs
        - img_name (str): Name of the input image
        """

        if output_path.exists():
            with open(output_path, mode='r', encoding='utf-8') as f:
                output_json = load(f)
        else:
            output_json = {}

        output_json[img_name] = strings

        with open(output_path, mode='w', encoding='utf-8') as f:
            dump(output_json, f, ensure_ascii=False)

    
    def _split_model(self):
        """
        Device map when running on multiple GPUs.
        REF: https://huggingface.co/OpenGVLab/InternVL2-8B#multiple-gpus
        
        * returns device_map (dict): Dictionary of each layer mapped to a devie
        """
        
        device_map = {}
        world_size = torch.cuda.device_count()
        num_layers = 60
        # Since the first GPU will be used for ViT, treat it as half a GPU.
        num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))
        num_layers_per_gpu = [num_layers_per_gpu] * world_size
        num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)
        for i, num_layer in enumerate(num_layers_per_gpu):
            for j in range(num_layer):
                device_map[f'language_model.model.layers.{j}'] = i
        device_map['vision_model'] = 0
        device_map['mlp1'] = 0
        device_map['language_model.model.tok_embeddings'] = 0
        device_map['language_model.model.embed_tokens'] = 0
        device_map['language_model.output'] = 0
        device_map['language_model.model.norm'] = 0
        device_map['language_model.lm_head'] = 0
        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0

        return device_map