from torch import bfloat16, cuda
from math import ceil
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig
from pathlib import Path
from json import load, dump
import logging
from re import findall

from pipeline.extract_content.processing_utils import load_image, postprocess_response

class ContentExtraction:
    """
    Class for content extraction using a large language model.
    """

    def __init__(self, max_new_tokens, cache_folder, load_in_8bit, logger):
        """
        Initialise content extraction object.
        If model files are not found in the cache folder, they are downloaded and saved for reuse.
        The large language models needs GPU to run.
        
        - max_new_tokens (int): Max new tokens generated by the llm
        - cache_folder (str): Folder path to store model cache files
        - load_in_8bit (bool): 8-bit quantization. If set to false, 4-bit quantization will be used.
        - logger (Logger): Logger object
        """

        model = 'OpenGVLab/InternVL2-8B'
        self.logger = logger

        model_cache_folder = Path.joinpath(Path(cache_folder), Path(model.replace(r'/', '-')))

        if load_in_8bit: quantization_config = BitsAndBytesConfig(load_in_8bit=True)
        else: quantization_config = BitsAndBytesConfig(load_in_4bit=True)

        try:
            self.llm = AutoModel.from_pretrained(
                model_cache_folder,
                torch_dtype=bfloat16,
                quantization_config=quantization_config,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                # device_map=self._split_model(),    # uncomment when running on multiple GPUs
                attn_implementation="flash_attention_2"
            )
        except EnvironmentError:
            self.logger.Log("Downloading LLM files!", logging.INFO)
            self.llm = AutoModel.from_pretrained(
                model,
                torch_dtype=bfloat16,
                quantization_config=quantization_config,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
                # device_map=self._split_model(),    # uncomment when running on multiple GPUs
                attn_implementation="flash_attention_2",
                cache_dir=model_cache_folder
            )
            self.llm.save_pretrained(model_cache_folder)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True, use_fast=False)

        self.generation_config = dict(max_new_tokens=max_new_tokens)

    def extract_content(self, table, use_pipeline_a):
        """
        Method to extract content from the image, and generate a pandas dataframe by passing the preprocessed image along with a prompt to the large language model.
        
        - table (PIL): PIL image of the cropped table.

        * returns dataframes (List): List of dataframes in the image.
        """

        pixel_values = load_image(table).to(bfloat16).cuda()
        
        if use_pipeline_a:
            question_a1 = "<image>\n Structure the text into a table as shown in the image. Don't include a preamble."
            markdown_response = self.llm.chat(self.tokenizer, pixel_values, question_a1, self.generation_config)
        else:
            question_b1 = "<image>\n Structure the text into a table as shown in the image. Return a json. Return multiple json if there are multiple tables. Don't include a preamble."
            json_response = self.llm.chat(self.tokenizer, pixel_values, question_b1, self.generation_config)
            self.logger.Log(json_response, logging.INFO)

            question_b2 = f"<json>{json_response}</json>\n Convert json to table."
            markdown_response = self.llm.chat(self.tokenizer, pixel_values, question_b2, self.generation_config)

        self.logger.Log(markdown_response, logging.INFO)

        dataframes = postprocess_response(markdown_response)

        # [self.logger.Log(f'FString:\n{fstring}', logging.INFO) for fstring in dataframes]
        
        return dataframes

    def save_table(self, table, output_folder, img_name, pipeline_name, i, unix_timestamp):
        """
        Save table in a csv file.
        
        - table (dataframe): Dataframe to be saved
        - output_folder (str): Folder path to store outputs
        - img_name (str): Name of the input image
        - pipeline_name (str): Name of the pipeline
        - i (int): table number
        - unix_timestamp (int): Make every file name unique using timestamp
        """
        output_path = Path.joinpath(Path(output_folder), Path(f'{img_name}_{pipeline_name}_table_{i}_{unix_timestamp}.csv'))

        header = True
        if len(findall('[-]*', table.iloc[0, 0])) > 0:
            table.drop(0, axis=0, inplace=True)
        else:
            header = False
        
        table.to_csv(output_path, header=header, index=False)

    def _split_model(self):
        """
        Device map when running on multiple GPUs.
        REF: https://huggingface.co/OpenGVLab/InternVL2-8B#multiple-gpus
        
        * returns device_map (dict): Dictionary of each layer mapped to a devie
        """
        
        device_map = {}
        world_size = cuda.device_count()
        num_layers = 60
        # Since the first GPU will be used for ViT, treat it as half a GPU.
        num_layers_per_gpu = ceil(num_layers / (world_size - 0.5))
        num_layers_per_gpu = [num_layers_per_gpu] * world_size
        num_layers_per_gpu[0] = ceil(num_layers_per_gpu[0] * 0.5)
        for i, num_layer in enumerate(num_layers_per_gpu):
            for j in range(num_layer):
                device_map[f'language_model.model.layers.{j}'] = i
        device_map['vision_model'] = 0
        device_map['mlp1'] = 0
        device_map['language_model.model.tok_embeddings'] = 0
        device_map['language_model.model.embed_tokens'] = 0
        device_map['language_model.output'] = 0
        device_map['language_model.model.norm'] = 0
        device_map['language_model.lm_head'] = 0
        device_map[f'language_model.model.layers.{num_layers - 1}'] = 0

        return device_map